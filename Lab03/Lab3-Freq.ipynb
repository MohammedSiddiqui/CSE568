{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentism, Samples, and the Bootstrap(with minimal changes from cs109 @ harvard university, fall 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Latex commands are defined here. Doubleclick to see.\n",
    "\n",
    "$\\newcommand{\\Ex}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\Cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}$\n",
    "$\\newcommand{\\indic}{\\mathbb{1}}$\n",
    "$\\newcommand{\\avg}{\\overline}$\n",
    "$\\newcommand{\\est}{\\hat}$\n",
    "$\\newcommand{\\trueval}[1]{#1^{*}}$\n",
    "$\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The %... is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n",
    "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AND MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we do this? Lets get some data...\n",
    "\n",
    "Forty-four babies -- a new record -- were born in one 24-hour period at\n",
    "the Mater Mothers' Hospital in Brisbane, Queensland, Australia, on\n",
    "December 18, 1997.  For each of the 44 babies, _The Sunday Mail_\n",
    "recorded the time of birth, the sex of the child, and the birth weight\n",
    "in grams. Also included is the number of minutes since midnight for\n",
    "each birth.\n",
    "\n",
    "REFERENCE:\n",
    "Steele, S. (December 21, 1997), \"Babies by the Dozen for Christmas:\n",
    "24-Hour Baby Boom,\" _The Sunday Mail_ (Brisbane), p. 7.\n",
    "\n",
    "\"Datasets\n",
    "and Stories\" article \"A Simple Dataset for Demonstrating Common\n",
    "Distributions\" in the _Journal of Statistics Education_ (Dunn 1999).\n",
    "\n",
    "Columns\n",
    "\n",
    "       1 -  8  Time of birth recorded on the 24-hour clock\n",
    "       9 - 16  Sex of the child (1 = girl, 2 = boy)\n",
    "      17 - 24  Birth weight in grams\n",
    "      25 - 32  Number of minutes after midnight of each birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"babyboom.dat.txt\", header=None, sep='\\s+', \n",
    "                   names=['24hrtime','sex','weight','minutes'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.minutes.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is data?\n",
    "\n",
    "In labs before, you have seen datasets. As so in the example above. You have seen probability distributions of this data. Calculated means. Calculated standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas code for the week\n",
    "\n",
    "We'll keep showing some different aspects of Pandas+Seaborn each week. For example, you can very easily calculate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this correlation is a statistic calculated only on this data...this sample of babies. I have not asked the question: what does this mean for the population of babies.\n",
    "\n",
    "I'd thought that there would be a greater correlation between weight and sex, but apparently its not at all big for babies. Telles you Idont know much about babies :-). Here's a plot more tohelp you in future homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(col=\"sex\", data=df, size=8)\n",
    "g.map(plt.hist, \"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples vs population\n",
    "\n",
    "But we have never aked ourselves the philosophical question: what is data? **Frequentist statistics** is one answer to this philosophical question. It treats data as a **sample** from an existing **population**.\n",
    "\n",
    "This notion is probably clearest to you from elections, where some companies like Zogby or CNN take polls. The sample in these polls maybe a 1000 people, but they \"represent\" the electoral population at large. We attempt to draw inferences about how the population will vote based on these samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "\n",
    "Let us characterize our particular sample statistically then, using a *probability distribution*\n",
    "\n",
    "\n",
    "#### The Exponential Distribution\n",
    "\n",
    "The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\n",
    "\n",
    "It takes the form:\n",
    "$$\n",
    "f(x;\\lambda) = \\begin{cases}\n",
    "\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n",
    "0 & x < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "From Wikipedia: *In probability theory, a Poisson process is a stochastic process which counts the number of events and the time that these events occur in a given time interval. The time between each pair of consecutive events has an exponential distribution with parameter $\\lambda$ and each of these inter-arrival times is assumed to be independent of other inter-arrival times. The process is named after the French mathematician SimÃ©on Denis Poisson and is a good model of radioactive decay, telephone calls and requests for a particular document on a web server, among many other phenomena.*\n",
    "\n",
    "In our example above, we have the arrival times of the babies. There is no reason to expect any specific clustering in time, so one could think of modelling the arrival of the babies via a poisson process.\n",
    "\n",
    "Furthermore, the Poisson distribution can be used to model the number of births each hour over the 24-hour period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x, l: l*np.exp(-l*x)*(x>0)\n",
    "xpts=np.arange(-2,3,0.1)\n",
    "plt.plot(xpts,f(xpts, 2),'o');\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"exponential pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "x = np.linspace(0,4, 100)\n",
    "colors=sns.color_palette()\n",
    "\n",
    "lambda_ = [0.5, 1, 2, 4]\n",
    "plt.figure(figsize=(12,4))\n",
    "for l,c in zip(lambda_,colors):\n",
    "    plt.plot(x, expon.pdf(x, scale=1./l), lw=2, \n",
    "                color=c, label = \"$\\lambda = %.1f$\"%l)\n",
    "    plt.fill_between(x, expon.pdf(x, scale=1./l), color=c, alpha = .33)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"PDF at $x$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.title(\"Probability density function of an Exponential random variable;\\\n",
    " differing $\\lambda$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we draw from this distribution?\n",
    "\n",
    "Lets use the built in machinery in `scipy.stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "plt.plot(xpts,expon.pdf(xpts, scale=1./2.),'o')\n",
    "plt.hist(expon.rvs(size=1000, scale=1./2.), normed=True, alpha=0.5, bins=30);\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"exponential pdf and samples(normalized)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scipy.stats`, you can alternatively create a frozen object, which holds values of things like the scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rv = expon(scale=0.5)\n",
    "plt.plot(xpts,rv.pdf(xpts),'o')\n",
    "plt.hist(rv.rvs(size=1000), normed=True, alpha=0.5, bins=30);\n",
    "plt.plot(xpts, rv.cdf(xpts));\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"exponential pdf, cdf and samples(normalized)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding our data using a distribution\n",
    "\n",
    "Lets play with our data a bit to understand it:\n",
    "\n",
    "The first birth occurred at 0005, and the last birth in the 24-hour period at 2355. Thus the 43 inter-birth times happened over a 1430-minute period, giving a theoretical mean of 1430/43 = 33.26 minutes between births.\n",
    "\n",
    "Lets plot a histogram of the inter-birth times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timediffs = df.minutes.diff()[1:]\n",
    "timediffs.hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean or of an exponentially distributed random variable X with rate parameter $\\lambda$ can be analytically calculated as\n",
    "\n",
    "$$\\Ex[X] = \\frac{1}{\\lambda}.$$\n",
    "\n",
    "This makes intuitive sense: if you get babies at an average rate of 2 per hour, then you can expect to wait half an hour for every baby.\n",
    "\n",
    "The variance of X is given by\n",
    "\n",
    "$$\\Var[X] = \\frac{1}{\\lambda^2}.$$\n",
    "\n",
    "so the standard deviatiation is equal to the mean, just as in the discrete Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_from_mean = 1./timediffs.mean()\n",
    "print lambda_from_mean, 1./lambda_from_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes=np.arange(0, 160, 5)\n",
    "rv = expon(scale=1./lambda_from_mean)\n",
    "plt.plot(minutes,rv.pdf(minutes),'o')\n",
    "timediffs.hist(normed=True, alpha=0.5);\n",
    "plt.xlabel(\"minutes\");\n",
    "plt.title(\"Normalized data and model for estimated $\\hat{\\lambda}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we just do? We made a 'point estimate' of the scale or rate parameter as a compression of our data. But what does it mean to make such a point estimate? The next section on **Frequentist Statistics** tells us. But first, lets see the Poisson Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside: The Poisson Distribution\n",
    "\n",
    "The *Poisson Distribution* is defined for all positive integers: \n",
    "\n",
    "$$P(Z=k)=\\frac{\\lambda^k e^{â\\lambda}}{k!}, k=0,1,2,... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "k = np.arange(15)\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, lambda_ in enumerate([1, 2, 4, 6]):\n",
    "    plt.plot(k, poisson.pmf(k, lambda_), '-o', label=lambda_, color=colors[i])\n",
    "    plt.fill_between(k, poisson.pmf(k, lambda_), color=colors[i], alpha=0.5)\n",
    "    plt.legend()\n",
    "plt.title(\"Poisson distribution\")\n",
    "plt.ylabel(\"PDF at $k$\")\n",
    "plt.xlabel(\"$k$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_hour = df.minutes // 60\n",
    "num_births_per_hour=df.groupby(per_hour).minutes.count()\n",
    "num_births_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_births_per_hour.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = np.arange(5)\n",
    "plt.figure(figsize=(12,8))\n",
    "tcount=num_births_per_hour.sum()\n",
    "plt.hist(num_births_per_hour, alpha=0.4,  lw=3, normed=True, label=\"normed hist\")\n",
    "sns.kdeplot(num_births_per_hour, label=\"kde\")\n",
    "plt.plot(k, poisson.pmf(k, num_births_per_hour.mean()), '-o',label=\"poisson\")\n",
    "plt.title(\"Baby births\")\n",
    "plt.xlabel(\"births per hour\")\n",
    "plt.ylabel(\"rate\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "how did we know that the sample mean was a good thing to use?\n",
    "\n",
    "One of the techniques used to estimate such parameters in frequentist statistics is **maximum likelihood estimation**. Briefly, the idea behind it is:\n",
    "\n",
    "The product \n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^n P(x_i | \\lambda)\n",
    "$$\n",
    "\n",
    "gives us a measure of how likely it is to observe values $x_1,...,x_n$ given the parameters $\\lambda$. Maximum likelihood fitting consists of choosing the appropriate \"likelihood\" function $L=P(X|\\lambda)$ to maximize for a given set of observations. How likely are the observations if the model is true?\n",
    "\n",
    "Often it is easier and numerically more stable to maximise the log likelyhood:\n",
    "\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i | \\lambda))\n",
    "$$\n",
    "\n",
    "In the case of the exponential distribution we have:\n",
    "\n",
    "$$\n",
    "\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n",
    "$$\n",
    "\n",
    "Maximizing this:\n",
    "\n",
    "$$\n",
    "\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n",
    "$$\n",
    "\n",
    "and thus:\n",
    "\n",
    "$$\n",
    "\\est{\\lambda_{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n",
    "$$\n",
    "\n",
    "which is identical to the simple estimator we used above. Usually one is not so lucky and one must use numerical optimization techniques.\n",
    "\n",
    "A crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FREQUENTIST STATISTICS \n",
    "\n",
    "In frequentist statistics, the data we have in hand, is viewed as a **sample** from a population. So if we want to estimate some parameter of the population, like say the mean, we estimate it on the sample.\n",
    "\n",
    "This is because we've been given only one sample. Ideally we'd want to see the population, but we have no such luck.\n",
    "\n",
    "The parameter estimate is computed by applying an estimator $F$ to some data $D$, so $\\est{\\lambda} = F(D)$. \n",
    "\n",
    "\n",
    "**The parameter is viewed as fixed and the data as random, which is the exact opposite of the Bayesian approach which you will learn later in this class. **\n",
    "\n",
    "For the babies, lets assume that an exponential distribution is a good description of the baby arrival process. Then we consider some larger population of babies from which this sample is drawn, there is some true $\\trueval{\\lambda}$ which defines it. We dont know this. The best we can do to start with is to estimate  a lambda from the data set we have, which we denote $\\est{\\lambda}$. \n",
    "\n",
    "Now, imagine that I let you peek at the entire population in this way: I gave you some M data sets **drawn** from the population, and you can now find the mean on each such dataset, of which the one we have here is one.\n",
    "So, we'd have M means. You can think of these means as coming from some fixed parameter by some data drawing process\n",
    "\n",
    "Now if we had many replications of this data set: that is, data from other days, an **ensemble** of data sets, for example, we can compute other $\\est{\\lambda}$, and begin to construct the **sampling distribution** of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue: many samples on the binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import bernoulli\n",
    "def throw_a_coin(n):\n",
    "    brv = bernoulli(0.5)\n",
    "    return brv.rvs(size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below returns the mean for each sample in an ensemble of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_throws(number_of_samples, sample_size):\n",
    "    start=np.zeros((number_of_samples, sample_size), dtype=int)\n",
    "    for i in range(number_of_samples):\n",
    "        start[i,:]=throw_a_coin(sample_size)\n",
    "    return np.mean(start, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample zise from 1 to 1000 in `sample_means`. This will rake some time to run as I am doing it for 200 replications at 1000 different sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_sizes=np.arange(1,1001,1)\n",
    "sample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So remember that for eachsample size, i am getting 200 means. Lets get the mean of the means at each sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_of_sample_means = [np.mean(means) for means in sample_means]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, mean_of_sample_means);\n",
    "plt.ylim([0.480,0.520]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large.\n",
    "\n",
    "#### The notion of a Sampling Distribution\n",
    "\n",
    "(some text is quoted from Murphy's machine learning book)\n",
    "\n",
    "In data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\n",
    "\n",
    "But some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a **sampling distribution**. \n",
    "\n",
    "Since, in the real world, we see only one sample, this distribution helps us do **inference**, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given, we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the **sampling distribution of the mean**, or the distribution of sample means.\n",
    "\n",
    "So, in the babies case, the uncertainty in the parameter estimate can be measured by computing the **sampling distribution** of the estimator. \n",
    "What you are doing is sampling many Data Sets $D_i$ from the true population (which we are not given you will argue, and you are right, but just wait a bit), say M of them, each of size N, from some true model $p(\\cdot|\\trueval{\\lambda})$. We will now calculate M $\\est{\\lambda}_i$, one for each dataset. As we let $M \\rightarrow \\infty$, the distribution induced on $\\est{\\lambda}$ is the sampling distribution of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "Just having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\n",
    "\n",
    "In the last two decades, **resampling** the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "\n",
    "Bootstrap tries to approximate our sampling distribution. If we knew the true parameters of the population, we could generate M fake datasets. Then we could compute the parameter (or another estimator) on each one of these, to get a empirical sampling distribution of the parameter or estimator, and which will give us an idea of how typical our sample is, and thus, how good our parameter estimations from our sample are.\n",
    "(again from murphy)\n",
    "\n",
    "But we dont have the true parameter. So we generate these samples, using the parameter we calculated. Or, alteratively, we sample with replacement the X from our original sample D, generating many fake datasets, and then compute the distribution on the parameters as before. \n",
    "\n",
    "We do it here for the mean of the time differences. We could also do it for its inverse, $\\lambda$.\n",
    "\n",
    "#### Non Parametric bootstrap\n",
    "\n",
    "Resample the data! We can then plot the distribution of the mean time-difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_samples=10000\n",
    "N_points = timediffs.shape[0]\n",
    "bs_np = np.random.choice(timediffs, size=(M_samples, N_points))\n",
    "sd_mean=np.mean(bs_np, axis=1)\n",
    "sd_std=np.std(bs_np, axis=1)\n",
    "plt.hist(sd_mean, bins=30, normed=True, alpha=0.5,label=\"samples\");\n",
    "sns.kdeplot(sd_mean, label=\"inferred distribution\")\n",
    "plt.axvline(timediffs.mean(), 0, 1, color='r', label='Our Sample')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametric Bootstrap\n",
    "\n",
    "And here we do it in a parametric way. We get an \"estimate\" of the parameter from our sample, and them use the exponential distribution to generate many datasets, and then fir the parameter on each one of those datasets. We can then plot the distribution of the mean time-difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rv = expon(scale=1./lambda_from_mean)\n",
    "M_samples=10000\n",
    "N_points = timediffs.shape[0]\n",
    "bs_p = rv.rvs(size=(M_samples, N_points))\n",
    "sd_mean_p=np.mean(bs_p, axis=1)\n",
    "sd_std_p=np.std(bs_p, axis=1)\n",
    "plt.hist(sd_mean_p, bins=30, normed=True, alpha=0.5);\n",
    "sns.kdeplot(sd_mean_p);\n",
    "plt.axvline(timediffs.mean(), 0, 1, color='r', label='Our Sample')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
